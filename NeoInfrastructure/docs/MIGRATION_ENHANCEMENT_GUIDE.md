# Migration System Enhancement Guide

This guide provides detailed instructions for enhancing the existing migration system to support event-driven triggers, improved monitoring, and advanced operational capabilities.

## ðŸŽ¯ Enhancement Overview

### Current State Analysis
The existing migration system has these components:
- `DynamicMigrationEngine`: Handles dynamic migration execution based on database connections
- `EnhancedMigrationManager`: Manages migration operations with locking and status tracking
- `MigrationDependencyResolver`: Resolves migration dependencies and order

### Enhancement Goals
1. **Event-Driven Architecture**: Auto-trigger migrations on database/tenant creation
2. **Real-Time Monitoring**: Live progress tracking and performance metrics
3. **Advanced Rollback**: Safe rollback with state preservation
4. **Smart Retry Logic**: Intelligent retry with exponential backoff
5. **Batch Optimization**: Improved parallel processing for bulk operations
6. **Health Monitoring**: Comprehensive system health checks

---

## ðŸ—ï¸ Enhanced Architecture

### Event-Driven Migration System
```
Event Sources:
â”œâ”€â”€ Database Connection Created â†’ Trigger Database Migration
â”œâ”€â”€ Tenant Created â†’ Trigger Tenant Schema Migration
â”œâ”€â”€ Scheduled Maintenance â†’ Trigger Batch Migrations
â”œâ”€â”€ Manual Request â†’ Trigger Specific Migration
â””â”€â”€ System Recovery â†’ Trigger Rollback Operations

Event Processing:
â”œâ”€â”€ Event Queue (Redis/Database)
â”œâ”€â”€ Event Handlers (Async processors)
â”œâ”€â”€ Dependency Resolution
â”œâ”€â”€ Execution Planning
â””â”€â”€ Progress Tracking
```

### Enhanced Components Structure
```
src/features/migrations/
â”œâ”€â”€ engines/                     # Enhanced migration engines
â”‚   â”œâ”€â”€ dynamic_migration_engine.py
â”‚   â”œâ”€â”€ enhanced_migration_manager.py
â”‚   â”œâ”€â”€ migration_dependency_resolver.py
â”‚   â””â”€â”€ event_driven_engine.py   # NEW: Event processing
â”‚
â”œâ”€â”€ services/                    # Business logic services
â”‚   â”œâ”€â”€ migration_service.py
â”‚   â”œâ”€â”€ event_service.py         # NEW: Event handling
â”‚   â”œâ”€â”€ monitoring_service.py    # NEW: Real-time monitoring
â”‚   â””â”€â”€ rollback_service.py      # NEW: Advanced rollback
â”‚
â”œâ”€â”€ events/                      # NEW: Event system
â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”œâ”€â”€ database_created_handler.py
â”‚   â”‚   â”œâ”€â”€ tenant_created_handler.py
â”‚   â”‚   â””â”€â”€ maintenance_handler.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ events.py
â”‚   â”‚   â””â”€â”€ payloads.py
â”‚   â””â”€â”€ queue/
â”‚       â”œâ”€â”€ redis_queue.py
â”‚       â””â”€â”€ database_queue.py
â”‚
â”œâ”€â”€ monitoring/                  # NEW: Enhanced monitoring
â”‚   â”œâ”€â”€ progress_tracker.py
â”‚   â”œâ”€â”€ performance_monitor.py
â”‚   â”œâ”€â”€ health_checker.py
â”‚   â””â”€â”€ alerting.py
â”‚
â””â”€â”€ utils/                       # Enhanced utilities
    â”œâ”€â”€ retry_logic.py
    â”œâ”€â”€ circuit_breaker.py
    â””â”€â”€ batch_optimizer.py
```

---

## ðŸ”„ Event-Driven Migration System

### Step 1: Event System Implementation

#### Task 1.1: Event Models and Types (1 day)
**File**: `src/features/migrations/events/models/events.py`

```python
"""
Event models for migration system.
"""
from typing import Dict, Any, Optional, List
from datetime import datetime
from enum import Enum
from pydantic import BaseModel, Field
from uuid import UUID, uuid4

class EventType(str, Enum):
    """Migration event types."""
    DATABASE_CREATED = "database.created"
    DATABASE_UPDATED = "database.updated"
    DATABASE_DELETED = "database.deleted"
    TENANT_CREATED = "tenant.created"
    TENANT_UPDATED = "tenant.updated"
    TENANT_DELETED = "tenant.deleted"
    MAINTENANCE_SCHEDULED = "maintenance.scheduled"
    MIGRATION_REQUESTED = "migration.requested"
    ROLLBACK_REQUESTED = "rollback.requested"

class EventPriority(str, Enum):
    """Event processing priority."""
    LOW = "low"
    NORMAL = "normal"
    HIGH = "high"
    CRITICAL = "critical"

class EventStatus(str, Enum):
    """Event processing status."""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRYING = "retrying"

class MigrationEvent(BaseModel):
    """Base migration event model."""
    id: UUID = Field(default_factory=uuid4)
    event_type: EventType
    priority: EventPriority = EventPriority.NORMAL
    status: EventStatus = EventStatus.PENDING
    payload: Dict[str, Any]
    metadata: Dict[str, Any] = Field(default_factory=dict)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    scheduled_at: Optional[datetime] = None
    processed_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    retry_count: int = 0
    max_retries: int = 3
    error_message: Optional[str] = None
    correlation_id: Optional[str] = None
    triggered_by: Optional[str] = None

class DatabaseCreatedEvent(BaseModel):
    """Event payload for database creation."""
    database_id: str
    database_name: str
    connection_type: str
    region: str
    host: str
    port: int
    username: str
    auto_migrate: bool = True
    migration_timeout: int = 300

class TenantCreatedEvent(BaseModel):
    """Event payload for tenant creation."""
    tenant_id: str
    tenant_slug: str
    database_id: str
    schema_name: str
    region: str
    template_version: Optional[str] = None
    auto_migrate: bool = True

class MaintenanceScheduledEvent(BaseModel):
    """Event payload for scheduled maintenance."""
    maintenance_id: str
    maintenance_type: str
    target_databases: List[str]
    target_schemas: List[str]
    scheduled_time: datetime
    maintenance_window: int  # minutes
    auto_rollback: bool = True

class MigrationRequestedEvent(BaseModel):
    """Event payload for manual migration request."""
    request_id: str
    scope: str
    target_databases: Optional[List[str]] = None
    target_schemas: Optional[List[str]] = None
    dry_run: bool = False
    force_execution: bool = False
    requested_by: str
```

#### Task 1.2: Event Queue Implementation (1 day)
**File**: `src/features/migrations/events/queue/redis_queue.py`

```python
"""
Redis-based event queue for migration events.
"""
import json
import asyncio
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
import redis.asyncio as redis
from loguru import logger

from src.features.migrations.events.models.events import MigrationEvent, EventStatus, EventPriority
from src.common.config.settings import settings

class RedisEventQueue:
    """Redis-based event queue for migration events."""
    
    def __init__(self):
        self.redis_client: Optional[redis.Redis] = None
        self.queue_prefix = "migration_events"
        self.processing_timeout = 300  # 5 minutes
    
    async def initialize(self):
        """Initialize Redis connection."""
        try:
            self.redis_client = redis.from_url(
                settings.redis_url,
                decode_responses=True,
                socket_timeout=10
            )
            await self.redis_client.ping()
            logger.info("Redis event queue initialized")
        except Exception as e:
            logger.error(f"Failed to initialize Redis event queue: {e}")
            raise
    
    async def enqueue_event(self, event: MigrationEvent) -> bool:
        """Add event to queue based on priority."""
        
        if not self.redis_client:
            await self.initialize()
        
        try:
            # Serialize event
            event_data = {
                "event": event.dict(),
                "enqueued_at": datetime.utcnow().isoformat()
            }
            
            # Determine queue based on priority
            queue_name = f"{self.queue_prefix}:{event.priority.value}"
            
            # Add to queue with score for ordering
            score = self._calculate_priority_score(event)
            
            await self.redis_client.zadd(
                queue_name,
                {json.dumps(event_data): score}
            )
            
            # Update event tracking
            await self._track_event(event)
            
            logger.info(f"Event {event.id} enqueued to {queue_name}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to enqueue event {event.id}: {e}")
            return False
    
    async def dequeue_event(self, priority: Optional[EventPriority] = None) -> Optional[MigrationEvent]:
        """Dequeue next event for processing."""
        
        if not self.redis_client:
            return None
        
        try:
            # Determine which queues to check
            if priority:
                queues = [f"{self.queue_prefix}:{priority.value}"]
            else:
                # Check in priority order: critical, high, normal, low
                queues = [
                    f"{self.queue_prefix}:critical",
                    f"{self.queue_prefix}:high",
                    f"{self.queue_prefix}:normal",
                    f"{self.queue_prefix}:low"
                ]
            
            for queue_name in queues:
                # Get highest priority event
                result = await self.redis_client.zpopmin(queue_name, 1)
                
                if result:
                    event_json, score = result[0]
                    event_data = json.loads(event_json)
                    
                    # Deserialize event
                    event = MigrationEvent(**event_data["event"])
                    
                    # Mark as processing
                    event.status = EventStatus.PROCESSING
                    event.processed_at = datetime.utcnow()
                    
                    # Add to processing set with timeout
                    await self._mark_processing(event)
                    
                    logger.info(f"Event {event.id} dequeued for processing")
                    return event
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to dequeue event: {e}")
            return None
    
    async def complete_event(self, event: MigrationEvent, success: bool = True, error: Optional[str] = None):
        """Mark event as completed or failed."""
        
        try:
            # Update event status
            event.status = EventStatus.COMPLETED if success else EventStatus.FAILED
            event.completed_at = datetime.utcnow()
            if error:
                event.error_message = error
            
            # Remove from processing set
            await self._unmark_processing(event)
            
            # Update tracking
            await self._track_event(event)
            
            # Handle retry logic for failed events
            if not success and event.retry_count < event.max_retries:
                await self._schedule_retry(event)
            
            logger.info(f"Event {event.id} marked as {event.status.value}")
            
        except Exception as e:
            logger.error(f"Failed to complete event {event.id}: {e}")
    
    async def get_queue_stats(self) -> Dict[str, Any]:
        """Get queue statistics."""
        
        if not self.redis_client:
            return {}
        
        try:
            stats = {}
            
            for priority in EventPriority:
                queue_name = f"{self.queue_prefix}:{priority.value}"
                count = await self.redis_client.zcard(queue_name)
                stats[priority.value] = count
            
            # Processing count
            processing_key = f"{self.queue_prefix}:processing"
            processing_count = await self.redis_client.scard(processing_key)
            stats["processing"] = processing_count
            
            # Get oldest event in each queue
            for priority in EventPriority:
                queue_name = f"{self.queue_prefix}:{priority.value}"
                oldest = await self.redis_client.zrange(queue_name, 0, 0, withscores=True)
                if oldest:
                    stats[f"{priority.value}_oldest"] = oldest[0][1]  # Score (timestamp)
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get queue stats: {e}")
            return {}
    
    async def cleanup_stale_events(self):
        """Clean up stale processing events."""
        
        if not self.redis_client:
            return
        
        try:
            processing_key = f"{self.queue_prefix}:processing"
            processing_events = await self.redis_client.smembers(processing_key)
            
            cutoff_time = datetime.utcnow() - timedelta(seconds=self.processing_timeout)
            
            for event_id in processing_events:
                # Check if event is stale
                event_key = f"{self.queue_prefix}:event:{event_id}"
                event_data = await self.redis_client.hgetall(event_key)
                
                if event_data:
                    processed_at = datetime.fromisoformat(event_data.get("processed_at", ""))
                    
                    if processed_at < cutoff_time:
                        # Requeue stale event
                        await self._requeue_stale_event(event_id)
                        logger.warning(f"Requeued stale event: {event_id}")
            
        except Exception as e:
            logger.error(f"Failed to cleanup stale events: {e}")
    
    def _calculate_priority_score(self, event: MigrationEvent) -> float:
        """Calculate priority score for queue ordering."""
        
        base_scores = {
            EventPriority.CRITICAL: 1000000,
            EventPriority.HIGH: 100000,
            EventPriority.NORMAL: 10000,
            EventPriority.LOW: 1000
        }
        
        base_score = base_scores.get(event.priority, 10000)
        
        # Add timestamp for FIFO within priority
        timestamp_score = event.created_at.timestamp()
        
        return base_score + timestamp_score
    
    async def _track_event(self, event: MigrationEvent):
        """Track event status in Redis."""
        
        event_key = f"{self.queue_prefix}:event:{event.id}"
        
        await self.redis_client.hset(
            event_key,
            mapping={
                "status": event.status.value,
                "created_at": event.created_at.isoformat(),
                "processed_at": event.processed_at.isoformat() if event.processed_at else "",
                "completed_at": event.completed_at.isoformat() if event.completed_at else "",
                "retry_count": event.retry_count,
                "error_message": event.error_message or ""
            }
        )
        
        # Set expiry for tracking data (24 hours)
        await self.redis_client.expire(event_key, 86400)
    
    async def _mark_processing(self, event: MigrationEvent):
        """Mark event as being processed."""
        
        processing_key = f"{self.queue_prefix}:processing"
        await self.redis_client.sadd(processing_key, str(event.id))
    
    async def _unmark_processing(self, event: MigrationEvent):
        """Remove event from processing set."""
        
        processing_key = f"{self.queue_prefix}:processing"
        await self.redis_client.srem(processing_key, str(event.id))
    
    async def _schedule_retry(self, event: MigrationEvent):
        """Schedule event for retry with exponential backoff."""
        
        event.retry_count += 1
        event.status = EventStatus.RETRYING
        
        # Calculate delay (exponential backoff)
        delay_seconds = min(300, 2 ** event.retry_count * 60)  # Max 5 minutes
        event.scheduled_at = datetime.utcnow() + timedelta(seconds=delay_seconds)
        
        # Add to delayed queue
        delayed_queue = f"{self.queue_prefix}:delayed"
        await self.redis_client.zadd(
            delayed_queue,
            {json.dumps(event.dict()): event.scheduled_at.timestamp()}
        )
        
        logger.info(f"Event {event.id} scheduled for retry in {delay_seconds} seconds")
    
    async def _requeue_stale_event(self, event_id: str):
        """Requeue a stale event."""
        
        # Implementation would retrieve event and requeue it
        # This is a placeholder for the actual requeue logic
        pass

# Global queue instance
_event_queue: Optional[RedisEventQueue] = None

async def get_event_queue() -> RedisEventQueue:
    """Get initialized event queue instance."""
    global _event_queue
    
    if _event_queue is None:
        _event_queue = RedisEventQueue()
        await _event_queue.initialize()
    
    return _event_queue
```

#### Task 1.3: Event Handlers Implementation (2 days)
**File**: `src/features/migrations/events/handlers/database_created_handler.py`

```python
"""
Handler for database creation events.
"""
from typing import Dict, Any
from loguru import logger

from src.features.migrations.events.models.events import MigrationEvent, DatabaseCreatedEvent
from src.features.migrations.services.migration_service import get_migration_service
from src.features.migrations.models.domain import MigrationScope
from src.common.exceptions.base import MigrationException

class DatabaseCreatedHandler:
    """Handler for database creation events."""
    
    def __init__(self):
        self.migration_service = None
    
    async def initialize(self):
        """Initialize handler dependencies."""
        self.migration_service = get_migration_service()
    
    async def handle(self, event: MigrationEvent) -> bool:
        """
        Handle database creation event.
        
        Args:
            event: Migration event with database creation payload
            
        Returns:
            True if handled successfully, False otherwise
        """
        
        if not self.migration_service:
            await self.initialize()
        
        try:
            # Parse event payload
            payload = DatabaseCreatedEvent(**event.payload)
            
            logger.info(
                f"Handling database creation event for {payload.database_name}",
                extra={
                    "event_id": str(event.id),
                    "database_id": payload.database_id,
                    "database_name": payload.database_name,
                    "region": payload.region
                }
            )
            
            # Check if auto-migration is enabled
            if not payload.auto_migrate:
                logger.info(f"Auto-migration disabled for database {payload.database_name}")
                return True
            
            # Determine migration scope based on database type
            scope = self._determine_migration_scope(payload.connection_type)
            
            # Execute migration for the new database
            execution = await self.migration_service.execute_targeted_migration(
                scope=scope,
                target_databases=[payload.database_id],
                dry_run=False,
                executed_by=f"event:{event.id}",
                timeout=payload.migration_timeout,
                metadata={
                    "triggered_by": "database_created_event",
                    "event_id": str(event.id),
                    "auto_migration": True
                }
            )
            
            logger.info(
                f"Database migration triggered for {payload.database_name}",
                extra={
                    "execution_id": str(execution.id),
                    "database_id": payload.database_id,
                    "scope": scope.value
                }
            )
            
            # Store execution reference in event metadata
            event.metadata.update({
                "migration_execution_id": str(execution.id),
                "migration_scope": scope.value,
                "target_database": payload.database_name
            })
            
            return True
            
        except Exception as e:
            logger.error(
                f"Failed to handle database creation event: {e}",
                extra={
                    "event_id": str(event.id),
                    "error": str(e)
                }
            )
            raise MigrationException(
                f"Database creation event handling failed: {str(e)}",
                details={"event_id": str(event.id)}
            )
    
    def _determine_migration_scope(self, connection_type: str) -> MigrationScope:
        """Determine migration scope based on database connection type."""
        
        type_mapping = {
            "admin": MigrationScope.ADMIN,
            "shared": MigrationScope.REGIONAL,
            "analytics": MigrationScope.REGIONAL,
            "tenant": MigrationScope.TENANT
        }
        
        return type_mapping.get(connection_type.lower(), MigrationScope.REGIONAL)
    
    async def validate_event(self, event: MigrationEvent) -> bool:
        """Validate database creation event payload."""
        
        try:
            payload = DatabaseCreatedEvent(**event.payload)
            
            # Validate required fields
            if not payload.database_id or not payload.database_name:
                return False
            
            # Validate connection parameters
            if not payload.host or not payload.port:
                return False
            
            return True
            
        except Exception:
            return False
```

**File**: `src/features/migrations/events/handlers/tenant_created_handler.py`

```python
"""
Handler for tenant creation events.
"""
from typing import Dict, Any
from loguru import logger

from src.features.migrations.events.models.events import MigrationEvent, TenantCreatedEvent
from src.features.migrations.services.migration_service import get_migration_service
from src.common.exceptions.base import MigrationException

class TenantCreatedHandler:
    """Handler for tenant creation events."""
    
    def __init__(self):
        self.migration_service = None
    
    async def initialize(self):
        """Initialize handler dependencies."""
        self.migration_service = get_migration_service()
    
    async def handle(self, event: MigrationEvent) -> bool:
        """
        Handle tenant creation event.
        
        Args:
            event: Migration event with tenant creation payload
            
        Returns:
            True if handled successfully, False otherwise
        """
        
        if not self.migration_service:
            await self.initialize()
        
        try:
            # Parse event payload
            payload = TenantCreatedEvent(**event.payload)
            
            logger.info(
                f"Handling tenant creation event for {payload.tenant_slug}",
                extra={
                    "event_id": str(event.id),
                    "tenant_id": payload.tenant_id,
                    "tenant_slug": payload.tenant_slug,
                    "database_id": payload.database_id,
                    "schema_name": payload.schema_name
                }
            )
            
            # Check if auto-migration is enabled
            if not payload.auto_migrate:
                logger.info(f"Auto-migration disabled for tenant {payload.tenant_slug}")
                return True
            
            # Execute tenant schema migration
            execution = await self.migration_service.execute_tenant_migration(
                tenant_id=payload.tenant_id,
                database_id=payload.database_id,
                schema_name=payload.schema_name,
                template_version=payload.template_version,
                create_schema=True,
                executed_by=f"event:{event.id}",
                metadata={
                    "triggered_by": "tenant_created_event",
                    "event_id": str(event.id),
                    "auto_migration": True,
                    "tenant_slug": payload.tenant_slug
                }
            )
            
            logger.info(
                f"Tenant migration triggered for {payload.tenant_slug}",
                extra={
                    "execution_id": str(execution.id),
                    "tenant_id": payload.tenant_id,
                    "schema_name": payload.schema_name
                }
            )
            
            # Store execution reference in event metadata
            event.metadata.update({
                "migration_execution_id": str(execution.id),
                "tenant_id": payload.tenant_id,
                "schema_name": payload.schema_name,
                "tenant_slug": payload.tenant_slug
            })
            
            return True
            
        except Exception as e:
            logger.error(
                f"Failed to handle tenant creation event: {e}",
                extra={
                    "event_id": str(event.id),
                    "error": str(e)
                }
            )
            raise MigrationException(
                f"Tenant creation event handling failed: {str(e)}",
                details={"event_id": str(event.id)}
            )
    
    async def validate_event(self, event: MigrationEvent) -> bool:
        """Validate tenant creation event payload."""
        
        try:
            payload = TenantCreatedEvent(**event.payload)
            
            # Validate required fields
            required_fields = [
                payload.tenant_id,
                payload.tenant_slug,
                payload.database_id,
                payload.schema_name
            ]
            
            if not all(required_fields):
                return False
            
            # Validate schema name format
            if not payload.schema_name.isidentifier():
                return False
            
            return True
            
        except Exception:
            return False
```

---

### Step 2: Real-Time Monitoring System

#### Task 2.1: Progress Tracking Enhancement (1.5 days)
**File**: `src/features/migrations/monitoring/progress_tracker.py`

```python
"""
Real-time progress tracking for migration operations.
"""
import asyncio
from typing import Dict, Any, Optional, List, Callable
from datetime import datetime, timedelta
from uuid import UUID
from dataclasses import dataclass, field
from loguru import logger
import redis.asyncio as redis

from src.features.migrations.models.domain import MigrationExecution, MigrationProgress
from src.common.config.settings import settings

@dataclass
class ProgressUpdate:
    """Progress update data structure."""
    execution_id: UUID
    current_step: str
    completed_steps: int
    total_steps: int
    current_target: Optional[str] = None
    estimated_completion: Optional[datetime] = None
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    error_count: int = 0
    warnings: List[str] = field(default_factory=list)

class ProgressTracker:
    """Real-time progress tracker for migration operations."""
    
    def __init__(self):
        self.redis_client: Optional[redis.Redis] = None
        self.subscribers: Dict[str, List[Callable]] = {}
        self.active_executions: Dict[UUID, ProgressUpdate] = {}
        self.update_interval = 1  # seconds
    
    async def initialize(self):
        """Initialize progress tracker."""
        try:
            self.redis_client = redis.from_url(
                settings.redis_url,
                decode_responses=True
            )
            await self.redis_client.ping()
            logger.info("Progress tracker initialized")
        except Exception as e:
            logger.warning(f"Redis not available for progress tracking: {e}")
            self.redis_client = None
    
    async def start_tracking(self, execution: MigrationExecution) -> bool:
        """Start tracking progress for a migration execution."""
        
        try:
            progress_update = ProgressUpdate(
                execution_id=execution.id,
                current_step="initializing",
                completed_steps=0,
                total_steps=len(execution.targets),
                estimated_completion=self._estimate_completion(execution)
            )
            
            self.active_executions[execution.id] = progress_update
            
            # Store initial progress in Redis
            if self.redis_client:
                await self._store_progress(progress_update)
            
            logger.info(f"Started tracking progress for execution {execution.id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to start progress tracking: {e}")
            return False
    
    async def update_progress(
        self,
        execution_id: UUID,
        current_step: str,
        completed_steps: Optional[int] = None,
        current_target: Optional[str] = None,
        performance_metrics: Optional[Dict[str, Any]] = None,
        error_count: Optional[int] = None,
        warnings: Optional[List[str]] = None
    ) -> bool:
        """Update progress for a migration execution."""
        
        if execution_id not in self.active_executions:
            logger.warning(f"No active tracking for execution {execution_id}")
            return False
        
        try:
            progress = self.active_executions[execution_id]
            
            # Update progress data
            progress.current_step = current_step
            if completed_steps is not None:
                progress.completed_steps = completed_steps
            if current_target is not None:
                progress.current_target = current_target
            if performance_metrics:
                progress.performance_metrics.update(performance_metrics)
            if error_count is not None:
                progress.error_count = error_count
            if warnings:
                progress.warnings.extend(warnings)
            
            # Update estimated completion
            progress.estimated_completion = self._update_estimated_completion(progress)
            
            # Store updated progress
            if self.redis_client:
                await self._store_progress(progress)
            
            # Notify subscribers
            await self._notify_subscribers(execution_id, progress)
            
            logger.debug(f"Updated progress for execution {execution_id}: {current_step}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to update progress for {execution_id}: {e}")
            return False
    
    async def complete_tracking(self, execution_id: UUID, success: bool = True):
        """Complete progress tracking for a migration execution."""
        
        if execution_id not in self.active_executions:
            return
        
        try:
            progress = self.active_executions[execution_id]
            progress.current_step = "completed" if success else "failed"
            progress.estimated_completion = datetime.utcnow()
            
            # Final update
            if self.redis_client:
                await self._store_progress(progress, final=True)
            
            # Notify subscribers of completion
            await self._notify_subscribers(execution_id, progress)
            
            # Clean up active tracking
            del self.active_executions[execution_id]
            
            logger.info(f"Completed tracking for execution {execution_id}")
            
        except Exception as e:
            logger.error(f"Failed to complete tracking for {execution_id}: {e}")
    
    async def get_progress(self, execution_id: UUID) -> Optional[ProgressUpdate]:
        """Get current progress for a migration execution."""
        
        # Check active executions first
        if execution_id in self.active_executions:
            return self.active_executions[execution_id]
        
        # Check Redis for stored progress
        if self.redis_client:
            try:
                progress_data = await self.redis_client.hgetall(f"progress:{execution_id}")
                if progress_data:
                    return self._deserialize_progress(progress_data)
            except Exception as e:
                logger.debug(f"Failed to get progress from Redis: {e}")
        
        return None
    
    async def subscribe_to_progress(
        self,
        execution_id: UUID,
        callback: Callable[[ProgressUpdate], None]
    ):
        """Subscribe to progress updates for a specific execution."""
        
        execution_key = str(execution_id)
        if execution_key not in self.subscribers:
            self.subscribers[execution_key] = []
        
        self.subscribers[execution_key].append(callback)
        logger.debug(f"Added subscriber for execution {execution_id}")
    
    async def unsubscribe_from_progress(
        self,
        execution_id: UUID,
        callback: Callable[[ProgressUpdate], None]
    ):
        """Unsubscribe from progress updates."""
        
        execution_key = str(execution_id)
        if execution_key in self.subscribers:
            try:
                self.subscribers[execution_key].remove(callback)
                if not self.subscribers[execution_key]:
                    del self.subscribers[execution_key]
            except ValueError:
                pass
    
    async def get_all_active_progress(self) -> List[ProgressUpdate]:
        """Get progress for all active executions."""
        return list(self.active_executions.values())
    
    def _estimate_completion(self, execution: MigrationExecution) -> datetime:
        """Estimate completion time based on execution parameters."""
        
        # Base estimation: 30 seconds per target
        base_time_per_target = 30
        total_targets = len(execution.targets)
        
        # Adjust based on scope and complexity
        complexity_multiplier = {
            "ALL": 1.5,
            "ADMIN": 1.2,
            "REGIONAL": 1.3,
            "TENANT": 1.0
        }.get(execution.scope.value, 1.0)
        
        estimated_seconds = total_targets * base_time_per_target * complexity_multiplier
        return datetime.utcnow() + timedelta(seconds=estimated_seconds)
    
    def _update_estimated_completion(self, progress: ProgressUpdate) -> datetime:
        """Update estimated completion based on current progress."""
        
        if progress.completed_steps == 0:
            return progress.estimated_completion
        
        # Calculate rate of progress
        elapsed_time = datetime.utcnow() - progress.estimated_completion + timedelta(
            seconds=(progress.total_steps * 30)  # Original estimate
        )
        
        if progress.completed_steps > 0:
            rate = elapsed_time.total_seconds() / progress.completed_steps
            remaining_steps = progress.total_steps - progress.completed_steps
            estimated_remaining = timedelta(seconds=rate * remaining_steps)
            
            return datetime.utcnow() + estimated_remaining
        
        return progress.estimated_completion
    
    async def _store_progress(self, progress: ProgressUpdate, final: bool = False):
        """Store progress data in Redis."""
        
        if not self.redis_client:
            return
        
        try:
            progress_data = {
                "execution_id": str(progress.execution_id),
                "current_step": progress.current_step,
                "completed_steps": progress.completed_steps,
                "total_steps": progress.total_steps,
                "current_target": progress.current_target or "",
                "estimated_completion": progress.estimated_completion.isoformat() if progress.estimated_completion else "",
                "error_count": progress.error_count,
                "warnings": "|".join(progress.warnings),
                "updated_at": datetime.utcnow().isoformat(),
                "performance_metrics": str(progress.performance_metrics)
            }
            
            key = f"progress:{progress.execution_id}"
            await self.redis_client.hset(key, mapping=progress_data)
            
            # Set expiry
            ttl = 86400 if final else 3600  # 24 hours for final, 1 hour for active
            await self.redis_client.expire(key, ttl)
            
        except Exception as e:
            logger.debug(f"Failed to store progress in Redis: {e}")
    
    def _deserialize_progress(self, progress_data: Dict[str, str]) -> ProgressUpdate:
        """Deserialize progress data from Redis."""
        
        return ProgressUpdate(
            execution_id=UUID(progress_data["execution_id"]),
            current_step=progress_data["current_step"],
            completed_steps=int(progress_data["completed_steps"]),
            total_steps=int(progress_data["total_steps"]),
            current_target=progress_data["current_target"] or None,
            estimated_completion=datetime.fromisoformat(progress_data["estimated_completion"]) if progress_data["estimated_completion"] else None,
            error_count=int(progress_data["error_count"]),
            warnings=progress_data["warnings"].split("|") if progress_data["warnings"] else [],
            performance_metrics=eval(progress_data["performance_metrics"]) if progress_data["performance_metrics"] else {}
        )
    
    async def _notify_subscribers(self, execution_id: UUID, progress: ProgressUpdate):
        """Notify all subscribers of progress update."""
        
        execution_key = str(execution_id)
        if execution_key in self.subscribers:
            for callback in self.subscribers[execution_key]:
                try:
                    if asyncio.iscoroutinefunction(callback):
                        await callback(progress)
                    else:
                        callback(progress)
                except Exception as e:
                    logger.error(f"Error in progress callback: {e}")

# Global progress tracker
_progress_tracker: Optional[ProgressTracker] = None

async def get_progress_tracker() -> ProgressTracker:
    """Get initialized progress tracker instance."""
    global _progress_tracker
    
    if _progress_tracker is None:
        _progress_tracker = ProgressTracker()
        await _progress_tracker.initialize()
    
    return _progress_tracker
```

---

### Step 3: Advanced Rollback System

#### Task 3.1: Rollback Service Implementation (2 days)
**File**: `src/features/migrations/services/rollback_service.py`

```python
"""
Advanced rollback service for migration operations.
"""
from typing import Dict, Any, Optional, List
from uuid import UUID, uuid4
from datetime import datetime
from loguru import logger

from src.features.migrations.models.domain import (
    MigrationExecution, MigrationStatus, MigrationTarget, RollbackExecution
)
from src.features.migrations.repositories.migration_repository import MigrationRepository
from src.features.migrations.engines.dynamic_migration_engine import DynamicMigrationEngine
from src.common.exceptions.base import MigrationException

class RollbackStrategy:
    """Base rollback strategy."""
    
    async def can_rollback(self, execution: MigrationExecution) -> bool:
        """Check if rollback is possible for this execution."""
        raise NotImplementedError
    
    async def execute_rollback(
        self,
        execution: MigrationExecution,
        target_version: Optional[str] = None
    ) -> bool:
        """Execute rollback for the given execution."""
        raise NotImplementedError

class FlywaySafeRollbackStrategy(RollbackStrategy):
    """Safe rollback strategy using Flyway undo migrations."""
    
    async def can_rollback(self, execution: MigrationExecution) -> bool:
        """Check if Flyway undo migrations are available."""
        
        # Check if execution has completed successfully
        if execution.status != MigrationStatus.COMPLETED:
            return False
        
        # Check if undo migrations exist for the applied migrations
        # This would involve checking Flyway migration files
        return True  # Placeholder
    
    async def execute_rollback(
        self,
        execution: MigrationExecution,
        target_version: Optional[str] = None
    ) -> bool:
        """Execute Flyway undo rollback."""
        
        try:
            # Implementation would use Flyway undo command
            # This is a placeholder for actual Flyway undo logic
            logger.info(f"Executing Flyway undo rollback for {execution.id}")
            return True
            
        except Exception as e:
            logger.error(f"Flyway rollback failed: {e}")
            return False

class SnapshotRollbackStrategy(RollbackStrategy):
    """Rollback strategy using database snapshots."""
    
    async def can_rollback(self, execution: MigrationExecution) -> bool:
        """Check if snapshots are available for rollback."""
        
        # Check if pre-migration snapshots exist
        # This would involve checking snapshot storage
        return True  # Placeholder
    
    async def execute_rollback(
        self,
        execution: MigrationExecution,
        target_version: Optional[str] = None
    ) -> bool:
        """Execute snapshot-based rollback."""
        
        try:
            # Implementation would restore from snapshots
            # This is a placeholder for actual snapshot restore logic
            logger.info(f"Executing snapshot rollback for {execution.id}")
            return True
            
        except Exception as e:
            logger.error(f"Snapshot rollback failed: {e}")
            return False

class RollbackService:
    """Advanced rollback service with multiple strategies."""
    
    def __init__(self):
        self.repository = MigrationRepository()
        self.migration_engine = DynamicMigrationEngine()
        self.strategies = {
            "flyway_undo": FlywaySafeRollbackStrategy(),
            "snapshot": SnapshotRollbackStrategy()
        }
        self.default_strategy = "flyway_undo"
    
    async def can_rollback(
        self,
        execution_id: UUID,
        strategy: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Check if an execution can be rolled back.
        
        Args:
            execution_id: ID of the execution to check
            strategy: Rollback strategy to use
            
        Returns:
            Rollback capability information
        """
        
        execution = await self.repository.get_execution(execution_id)
        if not execution:
            return {
                "can_rollback": False,
                "reason": "Execution not found"
            }
        
        # Check execution status
        if execution.status != MigrationStatus.COMPLETED:
            return {
                "can_rollback": False,
                "reason": f"Execution status is {execution.status.value}, not completed"
            }
        
        # Check time window (e.g., can only rollback within 24 hours)
        time_limit = datetime.utcnow() - timedelta(hours=24)
        if execution.completed_at and execution.completed_at < time_limit:
            return {
                "can_rollback": False,
                "reason": "Rollback time window exceeded (24 hours)"
            }
        
        # Check strategies
        strategy_results = {}
        strategies_to_check = [strategy] if strategy else list(self.strategies.keys())
        
        for strategy_name in strategies_to_check:
            if strategy_name in self.strategies:
                try:
                    can_rollback = await self.strategies[strategy_name].can_rollback(execution)
                    strategy_results[strategy_name] = {
                        "available": can_rollback,
                        "reason": "Available" if can_rollback else "Not available"
                    }
                except Exception as e:
                    strategy_results[strategy_name] = {
                        "available": False,
                        "reason": f"Error checking strategy: {str(e)}"
                    }
        
        # Determine if any strategy is available
        any_available = any(result["available"] for result in strategy_results.values())
        
        return {
            "can_rollback": any_available,
            "execution_id": str(execution_id),
            "execution_status": execution.status.value,
            "completed_at": execution.completed_at.isoformat() if execution.completed_at else None,
            "strategies": strategy_results,
            "recommended_strategy": self._get_recommended_strategy(strategy_results)
        }
    
    async def execute_rollback(
        self,
        execution_id: UUID,
        strategy: Optional[str] = None,
        target_version: Optional[str] = None,
        executed_by: str = "system",
        dry_run: bool = False
    ) -> RollbackExecution:
        """
        Execute rollback for a migration execution.
        
        Args:
            execution_id: ID of the execution to rollback
            strategy: Rollback strategy to use
            target_version: Target version to rollback to
            executed_by: User/system executing the rollback
            dry_run: Whether to perform a dry run
            
        Returns:
            Rollback execution record
        """
        
        # Get original execution
        original_execution = await self.repository.get_execution(execution_id)
        if not original_execution:
            raise MigrationException(f"Execution {execution_id} not found")
        
        # Check if rollback is possible
        rollback_check = await self.can_rollback(execution_id, strategy)
        if not rollback_check["can_rollback"]:
            raise MigrationException(
                f"Cannot rollback execution {execution_id}: {rollback_check['reason']}"
            )
        
        # Create rollback execution record
        rollback_id = uuid4()
        rollback_execution = RollbackExecution(
            id=rollback_id,
            original_execution_id=execution_id,
            status=MigrationStatus.PENDING,
            strategy=strategy or rollback_check["recommended_strategy"],
            target_version=target_version,
            started_at=datetime.utcnow(),
            executed_by=executed_by,
            dry_run=dry_run,
            metadata={
                "original_execution": str(execution_id),
                "rollback_reason": "Manual rollback request"
            }
        )
        
        # Save rollback execution
        await self.repository.create_rollback_execution(rollback_execution)
        
        try:
            # Update status to in progress
            rollback_execution.status = MigrationStatus.IN_PROGRESS
            await self.repository.update_rollback_execution(rollback_execution)
            
            # Execute rollback strategy
            strategy_name = rollback_execution.strategy
            if strategy_name not in self.strategies:
                raise MigrationException(f"Unknown rollback strategy: {strategy_name}")
            
            logger.info(
                f"Starting rollback execution {rollback_id} using strategy {strategy_name}",
                extra={
                    "rollback_id": str(rollback_id),
                    "original_execution_id": str(execution_id),
                    "strategy": strategy_name,
                    "dry_run": dry_run
                }
            )
            
            if not dry_run:
                success = await self.strategies[strategy_name].execute_rollback(
                    original_execution,
                    target_version
                )
            else:
                # Dry run - just validate
                success = True
                logger.info(f"Dry run rollback completed for {rollback_id}")
            
            # Update rollback execution status
            rollback_execution.status = MigrationStatus.COMPLETED if success else MigrationStatus.FAILED
            rollback_execution.completed_at = datetime.utcnow()
            
            if success:
                # Mark original execution as rolled back
                original_execution.metadata["rolled_back"] = True
                original_execution.metadata["rolled_back_at"] = datetime.utcnow().isoformat()
                original_execution.metadata["rollback_execution_id"] = str(rollback_id)
                await self.repository.update_execution(original_execution)
                
                logger.info(f"Rollback execution {rollback_id} completed successfully")
            else:
                rollback_execution.error_message = "Rollback strategy execution failed"
                logger.error(f"Rollback execution {rollback_id} failed")
            
            await self.repository.update_rollback_execution(rollback_execution)
            return rollback_execution
            
        except Exception as e:
            # Update rollback execution with error
            rollback_execution.status = MigrationStatus.FAILED
            rollback_execution.error_message = str(e)
            rollback_execution.completed_at = datetime.utcnow()
            await self.repository.update_rollback_execution(rollback_execution)
            
            logger.error(f"Rollback execution {rollback_id} failed: {e}")
            raise MigrationException(
                f"Rollback execution failed: {str(e)}",
                details={"rollback_id": str(rollback_id)}
            )
    
    async def get_rollback_history(
        self,
        execution_id: Optional[UUID] = None,
        limit: int = 50
    ) -> List[RollbackExecution]:
        """Get rollback execution history."""
        
        return await self.repository.list_rollback_executions(
            original_execution_id=execution_id,
            limit=limit
        )
    
    def _get_recommended_strategy(self, strategy_results: Dict[str, Dict[str, Any]]) -> Optional[str]:
        """Get recommended rollback strategy based on availability."""
        
        # Priority order for strategies
        strategy_priority = ["flyway_undo", "snapshot"]
        
        for strategy in strategy_priority:
            if strategy in strategy_results and strategy_results[strategy]["available"]:
                return strategy
        
        return None

# Global service instance
_rollback_service: Optional[RollbackService] = None

def get_rollback_service() -> RollbackService:
    """Get rollback service instance."""
    global _rollback_service
    
    if _rollback_service is None:
        _rollback_service = RollbackService()
    
    return _rollback_service
```

---

### Step 4: Smart Retry and Circuit Breaker

#### Task 4.1: Retry Logic Implementation (1 day)
**File**: `src/features/migrations/utils/retry_logic.py`

```python
"""
Smart retry logic with exponential backoff and circuit breaker patterns.
"""
import asyncio
from typing import Callable, Any, Optional, List, Dict, Type
from datetime import datetime, timedelta
from enum import Enum
from dataclasses import dataclass
from loguru import logger

class RetryStrategy(str, Enum):
    """Retry strategy types."""
    EXPONENTIAL_BACKOFF = "exponential_backoff"
    LINEAR_BACKOFF = "linear_backoff"
    FIXED_DELAY = "fixed_delay"
    IMMEDIATE = "immediate"

class CircuitState(str, Enum):
    """Circuit breaker states."""
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing if service recovered

@dataclass
class RetryConfig:
    """Retry configuration."""
    max_attempts: int = 3
    initial_delay: float = 1.0
    max_delay: float = 60.0
    backoff_multiplier: float = 2.0
    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL_BACKOFF
    retryable_exceptions: List[Type[Exception]] = None
    circuit_breaker_enabled: bool = True
    circuit_breaker_threshold: int = 5
    circuit_breaker_timeout: float = 30.0

class CircuitBreaker:
    """Circuit breaker for preventing cascading failures."""
    
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: float = 30.0,
        expected_exception: Type[Exception] = Exception
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        self.failure_count = 0
        self.last_failure_time: Optional[datetime] = None
        self.state = CircuitState.CLOSED
    
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function with circuit breaker protection."""
        
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
                logger.info("Circuit breaker transitioning to HALF_OPEN")
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)
            self._on_success()
            return result
            
        except self.expected_exception as e:
            self._on_failure()
            raise e
    
    def _should_attempt_reset(self) -> bool:
        """Check if circuit breaker should attempt reset."""
        if self.last_failure_time is None:
            return True
        
        return datetime.utcnow() - self.last_failure_time > timedelta(seconds=self.recovery_timeout)
    
    def _on_success(self):
        """Handle successful operation."""
        self.failure_count = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        """Handle failed operation."""
        self.failure_count += 1
        self.last_failure_time = datetime.utcnow()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
            logger.warning(f"Circuit breaker opened after {self.failure_count} failures")

class SmartRetry:
    """Smart retry logic with various strategies."""
    
    def __init__(self, config: RetryConfig):
        self.config = config
        self.circuit_breaker = None
        
        if config.circuit_breaker_enabled:
            self.circuit_breaker = CircuitBreaker(
                failure_threshold=config.circuit_breaker_threshold,
                recovery_timeout=config.circuit_breaker_timeout
            )
    
    async def execute(
        self,
        func: Callable,
        *args,
        context: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> Any:
        """
        Execute function with retry logic.
        
        Args:
            func: Function to execute
            *args: Function arguments
            context: Execution context for logging
            **kwargs: Function keyword arguments
            
        Returns:
            Function result
            
        Raises:
            Exception: If all retry attempts fail
        """
        
        context = context or {}
        last_exception = None
        
        for attempt in range(1, self.config.max_attempts + 1):
            try:
                logger.debug(
                    f"Retry attempt {attempt}/{self.config.max_attempts}",
                    extra={"context": context, "attempt": attempt}
                )
                
                if self.circuit_breaker:
                    result = await self.circuit_breaker.call(func, *args, **kwargs)
                else:
                    result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)
                
                if attempt > 1:
                    logger.info(
                        f"Retry succeeded on attempt {attempt}",
                        extra={"context": context, "attempt": attempt}
                    )
                
                return result
                
            except Exception as e:
                last_exception = e
                
                # Check if exception is retryable
                if not self._is_retryable(e):
                    logger.error(
                        f"Non-retryable exception on attempt {attempt}: {e}",
                        extra={"context": context, "attempt": attempt}
                    )
                    raise e
                
                # Check if we should retry
                if attempt >= self.config.max_attempts:
                    logger.error(
                        f"All retry attempts exhausted. Final attempt {attempt} failed: {e}",
                        extra={"context": context, "attempt": attempt}
                    )
                    break
                
                # Calculate delay
                delay = self._calculate_delay(attempt)
                
                logger.warning(
                    f"Attempt {attempt} failed: {e}. Retrying in {delay:.2f}s",
                    extra={"context": context, "attempt": attempt, "delay": delay}
                )
                
                await asyncio.sleep(delay)
        
        # All attempts failed
        raise last_exception
    
    def _is_retryable(self, exception: Exception) -> bool:
        """Check if exception is retryable."""
        
        if self.config.retryable_exceptions is None:
            # Default retryable exceptions for database operations
            retryable_types = (
                ConnectionError,
                TimeoutError,
                OSError,
                # Add more specific database exceptions as needed
            )
            return isinstance(exception, retryable_types)
        
        return any(isinstance(exception, exc_type) for exc_type in self.config.retryable_exceptions)
    
    def _calculate_delay(self, attempt: int) -> float:
        """Calculate delay for next retry attempt."""
        
        if self.config.strategy == RetryStrategy.IMMEDIATE:
            return 0.0
        
        elif self.config.strategy == RetryStrategy.FIXED_DELAY:
            return self.config.initial_delay
        
        elif self.config.strategy == RetryStrategy.LINEAR_BACKOFF:
            delay = self.config.initial_delay * attempt
        
        elif self.config.strategy == RetryStrategy.EXPONENTIAL_BACKOFF:
            delay = self.config.initial_delay * (self.config.backoff_multiplier ** (attempt - 1))
        
        else:
            delay = self.config.initial_delay
        
        # Cap at max delay
        return min(delay, self.config.max_delay)

# Decorator for easy retry functionality
def retry_on_failure(
    max_attempts: int = 3,
    initial_delay: float = 1.0,
    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL_BACKOFF,
    retryable_exceptions: Optional[List[Type[Exception]]] = None,
    circuit_breaker: bool = True
):
    """
    Decorator for adding retry logic to functions.
    
    Args:
        max_attempts: Maximum number of retry attempts
        initial_delay: Initial delay between retries
        strategy: Retry strategy to use
        retryable_exceptions: List of exceptions that should trigger retry
        circuit_breaker: Whether to enable circuit breaker
    """
    
    def decorator(func: Callable):
        config = RetryConfig(
            max_attempts=max_attempts,
            initial_delay=initial_delay,
            strategy=strategy,
            retryable_exceptions=retryable_exceptions,
            circuit_breaker_enabled=circuit_breaker
        )
        
        retry_handler = SmartRetry(config)
        
        async def async_wrapper(*args, **kwargs):
            context = {
                "function": func.__name__,
                "module": func.__module__
            }
            return await retry_handler.execute(func, *args, context=context, **kwargs)
        
        def sync_wrapper(*args, **kwargs):
            context = {
                "function": func.__name__,
                "module": func.__module__
            }
            return asyncio.run(retry_handler.execute(func, *args, context=context, **kwargs))
        
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

# Migration-specific retry configurations
MIGRATION_RETRY_CONFIG = RetryConfig(
    max_attempts=3,
    initial_delay=2.0,
    max_delay=30.0,
    strategy=RetryStrategy.EXPONENTIAL_BACKOFF,
    circuit_breaker_enabled=True,
    circuit_breaker_threshold=5,
    circuit_breaker_timeout=60.0
)

DATABASE_CONNECTION_RETRY_CONFIG = RetryConfig(
    max_attempts=5,
    initial_delay=1.0,
    max_delay=10.0,
    strategy=RetryStrategy.EXPONENTIAL_BACKOFF,
    circuit_breaker_enabled=True,
    circuit_breaker_threshold=3,
    circuit_breaker_timeout=30.0
)

# Helper functions for common retry scenarios
async def retry_database_operation(func: Callable, *args, **kwargs) -> Any:
    """Retry database operation with appropriate configuration."""
    retry_handler = SmartRetry(DATABASE_CONNECTION_RETRY_CONFIG)
    return await retry_handler.execute(func, *args, **kwargs)

async def retry_migration_operation(func: Callable, *args, **kwargs) -> Any:
    """Retry migration operation with appropriate configuration."""
    retry_handler = SmartRetry(MIGRATION_RETRY_CONFIG)
    return await retry_handler.execute(func, *args, **kwargs)
```

---

## ðŸ“Š Implementation Timeline

### Week 1: Event System Foundation
- **Days 1-2**: Event models and queue implementation
- **Days 3-4**: Event handlers for database and tenant creation
- **Day 5**: Integration testing and basic event processing

### Week 2: Monitoring and Progress Tracking
- **Days 1-2**: Progress tracker implementation with Redis
- **Days 2-3**: Real-time monitoring and metrics collection
- **Days 4-5**: WebSocket/SSE integration for live updates

### Week 3: Advanced Features
- **Days 1-2**: Rollback service with multiple strategies
- **Days 3-4**: Smart retry logic and circuit breakers
- **Day 5**: Performance optimization and batch processing

### Week 4: Integration and Testing
- **Days 1-2**: Integration with existing migration engines
- **Days 3-4**: Comprehensive testing and validation
- **Day 5**: Documentation and deployment preparation

## ðŸ” Success Metrics

### Functional Metrics
- **Event Processing**: >99% event processing success rate
- **Auto-Migration**: Database/tenant creation triggers migration within 30 seconds
- **Progress Accuracy**: Real-time progress updates with <5 second latency
- **Rollback Success**: >95% rollback success rate for eligible migrations

### Performance Metrics
- **Event Latency**: <5 seconds from event creation to processing start
- **Migration Speed**: 30% improvement in bulk migration performance
- **Monitoring Overhead**: <5% performance impact from progress tracking
- **Retry Efficiency**: <10% of operations require retry

### Operational Metrics
- **System Uptime**: >99.9% uptime for migration services
- **Error Recovery**: Automatic recovery from 90% of transient failures
- **Monitoring Coverage**: 100% visibility into migration operations
- **Alert Response**: <5 minutes to detect and alert on migration failures

This comprehensive enhancement guide provides the foundation for building a robust, event-driven migration system with advanced monitoring, rollback capabilities, and operational excellence.